# Project Tech Stack

- **Backend**: Java 17, Spring Boot 3.2, Maven
- **Frontend**: Next.js 14, React 18, TypeScript, Tailwind CSS, Shadcn
- **Database**: PostgreSQL
- **Testing**: JUnit, Testcontainers (Backend); Jest, React Testing Library (Frontend)
- **DevOps**: Docker, Docker Compose, Kubernetes, Terraform, Terragrunt

# Development Workflow and Process

This section outlines the recommended end-to-end process for developing a modern, cloud-native, containerized full-stack application from initial concept through production deployment.

**Note:** This guide uses Google Cloud Platform (GCP) services (GKE, GCR, GCS, Cloud SQL, Secret Manager), but the same principles apply to other cloud providers:
- **AWS**: EKS, ECR, S3, RDS, Secrets Manager
- **Azure**: AKS, ACR, Blob Storage, Azure SQL, Key Vault
- **On-Premise**: Self-managed Kubernetes, Harbor/Nexus, MinIO, PostgreSQL, Vault

## Guiding Principles

- **Test Early, Test Often**: Write tests alongside feature development (TDD approach)
- **CI from Day One**: Initialize CI on first commit and evolve it with the project
- **Infrastructure as Code**: All infrastructure defined in version-controlled code
- **Security by Design**: Never commit secrets, use environment variables and secret managers
- **Dev-Prod Parity**: Development environment mirrors production as closely as possible

## Step-by-Step Development Process

### 1. Project Setup & MVP Foundation

**Objective**: Establish project structure and foundational setup

**Actions**:
- Initialize Git repository from the start
- Set up project structure:
  - `backend/` - Spring Boot application
  - `frontend/` - Next.js application
  - `.github/workflows/` - CI/CD pipelines (even if minimal)
  - `k8s/` - Kubernetes manifests (create stubs)
  - `terraform/` - Infrastructure code (create stubs)
- Create `.gitignore` ensuring `.env` files are excluded
- Create `.env.example` with all required variables (no actual secrets)
- Set up linting and code formatting (Prettier, ESLint, Checkstyle)
- **Initialize basic CI workflow immediately** - even if it just runs linting

**Deliverables**:
- ‚úÖ Git repository initialized with proper `.gitignore`
- ‚úÖ Project directory structure created
- ‚úÖ Basic CI workflow (`.github/workflows/ci.yml`) that runs on every commit
- ‚úÖ README.md with setup instructions
- ‚úÖ `.env.example` file committed

### 2. Containerization with Docker

**Objective**: Create Docker configurations for development and production

**Why Early?** Containerization ensures your development environment matches production from day one, preventing "works on my machine" issues.

**Actions**:
- Create `Dockerfile` for production builds (multi-stage for backend)
  - Backend: Use `maven:3.9-eclipse-temurin-17` for build, `eclipse-temurin:17-jre` for runtime
  - Frontend: Use `node:20-alpine`, build with `npm run build`, use Next.js standalone output
- Create `Dockerfile.dev` for development with hot-reload
  - Backend: Use Maven image, mount source as volumes, enable Spring DevTools
  - Frontend: Use Node image, mount source as volumes, run `npm run dev` with file watching
- Create `docker-compose.yml` for production-like local testing
- Create `docker-compose.dev.yml` for local development with hot-reload
- Create `.dockerignore` files to exclude unnecessary files from Docker context
- **Update CI workflow** to build Docker images and verify they work

## Docker Multi-Architecture Builds (CRITICAL)

**‚ö†Ô∏è ALWAYS build Docker images for the target platform architecture!**

### The Problem: Architecture Mismatch

**Common Error:**
```
exec /opt/java/openjdk/bin/java: exec format error
```

**Cause:** Image built for ARM64 (Apple Silicon Mac) but deployed to AMD64 (GKE/cloud servers)

### Solution: Use `--platform` Flag

**‚ùå WRONG - Builds for host architecture (ARM64 on Apple Silicon):**
```bash
docker build -t myimage:latest .
```

**‚úÖ CORRECT - Explicitly specify target platform:**
```bash
# For cloud deployments (GKE, EKS, AKS, most VPS)
docker build --platform linux/amd64 -t myimage:latest .

# For Apple Silicon Macs (local development)
docker build --platform linux/arm64 -t myimage:latest .
```

### Default Architecture by Platform

| Platform | Default Architecture | Notes |
|----------|---------------------|-------|
| **GKE (Google Kubernetes Engine)** | `linux/amd64` | Standard x86_64 nodes |
| **EKS (AWS)** | `linux/amd64` | Most EC2 instances |
| **AKS (Azure)** | `linux/amd64` | Standard VMs |
| **Apple Silicon Mac (M1/M2/M3)** | `linux/arm64` | Local development |
| **Intel Mac** | `linux/amd64` | Local development |
| **Most Cloud VPS** | `linux/amd64` | DigitalOcean, Linode, etc. |

### Best Practices

**1. Specify Platform in Dockerfile (Recommended):**

**‚úÖ CORRECT - Platform in Dockerfile:**
```dockerfile
# Multi-stage build with platform specified
FROM --platform=linux/amd64 maven:3.9-eclipse-temurin-17 AS build
WORKDIR /app
COPY pom.xml .
RUN mvn dependency:go-offline -B
COPY src ./src
RUN mvn clean package -DskipTests

FROM --platform=linux/amd64 eclipse-temurin:17-jre
WORKDIR /app
COPY --from=build /app/target/*.jar app.jar
ENTRYPOINT ["java", "-jar", "app.jar"]
```

**Then build normally:**
```bash
# No --platform flag needed!
docker build -t us-central1-docker.pkg.dev/PROJECT_ID/REPO/backend:TAG .
```

**Why this is better:**
- ‚úÖ No need to remember `--platform` flag
- ‚úÖ Consistent across all developers
- ‚úÖ Works automatically in CI/CD
- ‚úÖ Self-documenting (Dockerfile shows target platform)

---

**2. Alternative: Platform in Build Command (Less Recommended):**

If you can't modify the Dockerfile, use the flag:
```bash
docker build --platform linux/amd64 -t myimage:latest .
```

**Downside:** Easy to forget, inconsistent builds.

---

**3. Multi-Architecture Builds (Advanced):**
```bash
# Build for both AMD64 and ARM64
docker buildx build --platform linux/amd64,linux/arm64 \
  -t myimage:latest --push .
```

---

**4. CI/CD Pipelines:**
```yaml
# GitHub Actions example (platform in Dockerfile)
- name: Build Docker image
  run: |
    docker build -t $IMAGE_NAME:$TAG .
```

---

**5. Verify Image Architecture:**
```bash
# Check what architecture an image was built for
docker image inspect IMAGE_NAME:TAG | grep Architecture

# Should show: "Architecture": "amd64"
```

### Why This Matters

- **Performance**: Running ARM64 images on AMD64 (or vice versa) requires emulation (slow)
- **Compatibility**: Some images won't run at all (exec format error)
- **Production Parity**: Dev environment should match production architecture

### Quick Reference

**Recommended Approach (Platform in Dockerfile):**
```dockerfile
# In your Dockerfile
FROM --platform=linux/amd64 maven:3.9-eclipse-temurin-17 AS build
FROM --platform=linux/amd64 eclipse-temurin:17-jre
```

```bash
# Then build normally
docker build -t IMAGE .  # ‚úÖ Always builds AMD64
```

**Alternative Approach (Platform in Command):**
```bash
# ‚úÖ Building for cloud deployment (from any machine)
docker build --platform linux/amd64 -t IMAGE .

# ‚úÖ Building for local Apple Silicon
docker build --platform linux/arm64 -t IMAGE .

# ‚úÖ Building for both (requires buildx)
docker buildx build --platform linux/amd64,linux/arm64 -t IMAGE .

# ‚ùå NEVER do this for cloud deployments from Apple Silicon
docker build -t IMAGE .  # Builds ARM64, will fail on AMD64 servers
```

**Deliverables**:
- ‚úÖ `Dockerfile` and `Dockerfile.dev` for both backend and frontend
- ‚úÖ `docker-compose.yml` and `docker-compose.dev.yml`
- ‚úÖ `.dockerignore` files
- ‚úÖ CI builds and tests Docker images
- ‚úÖ Local development using `docker-compose -f docker-compose.dev.yml up`

### 3. Testing Strategy (Integrated Throughout Development)

**Objective**: Establish comprehensive testing approach

**Why Integrated?** Testing should be part of feature development, not an afterthought. Write tests as you build features.

**Actions**:
- **Backend Testing**:
  - Set up JUnit 5 and Mockito for unit tests
  - Configure Testcontainers for integration tests with real PostgreSQL
  - Use `@DataJpaTest` for repository tests
  - Use `@SpringBootTest` for full integration tests
  - Add `@WebMvcTest` for controller tests
- **Frontend Testing**:
  - Set up Jest and React Testing Library
  - Write component tests for UI components
  - Add integration tests for API interactions
  - Use Mock Service Worker (MSW) for API mocking
- **E2E Testing** (Optional, for critical flows):
  - Set up Playwright or Cypress
  - Test critical user journeys
- **Update CI** to run all tests on every commit
  - Unit tests: Fast feedback
  - Integration tests: Run with Testcontainers
  - E2E tests: Run on staging deployments

**Testing Philosophy**:
- Write tests before or alongside feature code (TDD)
- Aim for high code coverage but focus on meaningful tests
- Test behavior, not implementation details
- Keep tests fast and deterministic

**Test Coverage Requirements by Layer**:
- **Controllers** - MUST have tests using `@WebMvcTest` with mocked services
  - Test all endpoints (GET, POST, PUT, DELETE, PATCH)
  - Test request/response mapping
  - Test HTTP status codes
  - Example: `ExpenseControllerTest`, `BudgetControllerTest`
- **Services** - MUST have tests using `@ExtendWith(MockitoExtension.class)` with mocked repositories
  - Test all business logic methods
  - Test both success and failure paths
  - Test exception handling
  - Example: `ExpenseServiceTest`, `BudgetServiceTest`
- **Repositories** - Test custom queries using `@DataJpaTest` (if custom queries exist)
  - Spring Data JPA methods don't need tests (framework-tested)
  - Only test custom `@Query` methods
- **DO NOT test**: DTOs, Models, Config classes, Main Application class (excluded from coverage)

**Code Coverage Requirements**:
- **Backend (Java/Spring Boot)**: Minimum 70% line coverage, 50% branch coverage
  - Enforced via JaCoCo Maven plugin
  - CI pipeline fails if coverage thresholds not met
  - Gradually increase thresholds as test coverage improves (target: 80% lines, 70% branches)
  - **Excluded from coverage**: DTOs (`**/dto/**`), Models (`**/model/**`), Config (`**/config/**`), Main Application class
    - These are data containers with minimal logic (mostly Lombok-generated getters/setters)
    - Focus coverage on business logic: Services, Controllers, Repositories
- **Frontend (React/Next.js)**: Minimum 60% lines, 40% branches, 55% functions, 60% statements
  - Enforced via Jest coverage thresholds in `jest.config.js`
  - CI pipeline fails if coverage thresholds not met
  - Gradually increase thresholds as test coverage improves (target: 70% lines, 50% branches)
  - **Excluded from coverage**: Components (`src/components/**`), App directory (`src/app/**`)
    - Components will be tested separately with React Testing Library
    - Focus initial coverage on utility functions and API layer
- Coverage reports uploaded as artifacts in GitHub Actions
- Focus on meaningful tests over arbitrary coverage numbers
- **Branch coverage** is harder to achieve - requires testing all conditional paths (if/else, switch, ternary)

**Deliverables**:
- ‚úÖ Test frameworks configured for backend and frontend
- ‚úÖ Tests run automatically in CI on every commit
- ‚úÖ Test coverage reports generated
- ‚úÖ Integration tests use Testcontainers for production parity
- ‚úÖ Code coverage thresholds enforced in CI pipeline

### 4. Continuous Integration (CI) Pipeline

**Objective**: Automated build, test, and validation on every commit

**Why Early?** CI should start simple on day one and grow with the project. It provides immediate feedback and prevents integration issues.

**Actions**:
- Expand `.github/workflows/ci.yml`:
  - Checkout code
  - Set up Java and Node.js
  - Run linters (ESLint, Checkstyle)
  - Run unit tests
  - Run integration tests
  - Build Docker images
  - Run security scans (Trivy for containers, OWASP Dependency-Check)
  - Upload test coverage reports
- Configure CI to run on:
  - Every push to `main`, `develop`, and `feature/**` branches
  - Every pull request to `main` and `develop`
  - Scheduled runs (daily/weekly for dependency checks)
- Set branch protection rules:
  - Require CI to pass before merging
  - Require code reviews
  - Prevent force pushes to `main` and `develop`

**Deliverables**:
- ‚úÖ Comprehensive CI pipeline in `.github/workflows/ci.yml`
- ‚úÖ CI runs on all commits and pull requests
- ‚úÖ Branch protection rules enforced
- ‚úÖ Security scanning integrated
- ‚úÖ Failed tests block PR merges

### 5. Infrastructure as Code (Terraform + Terragrunt)

**Objective**: Define all cloud infrastructure in version-controlled code

**Why Before K8s?** You need infrastructure (VPC, subnets, managed K8s cluster, databases) before you can deploy Kubernetes workloads.

**Actions**:
- Set up Terraform project structure:
terraform/ 
‚îú‚îÄ‚îÄ environments/ 
‚îÇ ‚îú‚îÄ‚îÄ staging/ 
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ terragrunt.hcl 
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ terraform.tfvars 
‚îÇ ‚îî‚îÄ‚îÄ production/ 
‚îÇ ‚îú‚îÄ‚îÄ terragrunt.hcl 
‚îÇ ‚îî‚îÄ‚îÄ terraform.tfvars 
‚îú‚îÄ‚îÄ modules/ 
‚îÇ ‚îú‚îÄ‚îÄ networking/ 
‚îÇ ‚îú‚îÄ‚îÄ gke/ 
‚îÇ ‚îú‚îÄ‚îÄ database/ 
‚îÇ ‚îî‚îÄ‚îÄ storage/ 
‚îú‚îÄ‚îÄ backend.tf # Remote state configuration 
‚îî‚îÄ‚îÄ versions.tf

- **Use Terragrunt for multi-environment support**:
  - DRY principle: Write Terraform once, configure per environment with Terragrunt
  - Separate state files per environment
  - Environment-specific variables (cluster size, database tier, etc.)
- Define infrastructure resources:
  - VPC and subnets
  - GKE cluster (or EKS/AKS for AWS/Azure)
  - Cloud SQL (managed PostgreSQL)
  - GCS buckets for storage and Terraform state
  - IAM roles and service accounts
  - Secret Manager for secrets
- Configure remote state backend (GCS with locking)
- Create `.github/workflows/terraform-plan.yml` for PR validation
- Create `.github/workflows/terraform-apply.yml` for applying changes
- **Never apply Terraform manually** - always through CI/CD

**Terragrunt Benefits**:
- Keeps Terraform code DRY across environments
- Manages remote state configuration automatically
- Supports dependencies between modules
- Easier to scale to multiple projects/teams

**Deliverables**:
- ‚úÖ Terraform modules for all infrastructure components (GKE, Cloud SQL, VPC, Secret Manager)
- ‚úÖ Terragrunt configurations for staging and production
- ‚úÖ Remote state backend configured (GCS)
- ‚úÖ `.github/workflows/terraform-plan.yml` for PR validation
- ‚úÖ `.github/workflows/terraform-apply.yml` for automated deployment
- ‚úÖ `terraform/setup.sh` automated setup script
- ‚úÖ `INFRASTRUCTURE.md` quick start guide
- ‚úÖ `NETWORKING.md` comprehensive networking guide
- ‚úÖ `TERRAGRUNT_EXPLAINED.md` Terragrunt concepts explained
- ‚úÖ `GITHUB_WORKFLOWS_EXPLAINED.md` CI/CD workflows explained
- ‚úÖ Environment-specific configs: staging (preemptible, small), production (standard, HA)
- ‚úÖ Infrastructure provisioned and running

## Terraform & Terragrunt Best Practices

### Common Pitfalls & Solutions (Lessons Learned)

#### **1. GKE Cluster Configuration Issues**

**‚ùå WRONG - Deprecated/Invalid Configurations:**
```hcl
# Don't use these - they cause "invalid argument" errors
resource "google_container_cluster" "primary" {
  # ‚ùå network_policy block is deprecated
  network_policy {
    enabled = true
  }
  
  # ‚ùå Invalid CIDR notation
  ip_allocation_policy {
    cluster_ipv4_cidr_block  = "/16"  # Missing IP prefix!
    services_ipv4_cidr_block = "/22"
  }
  
  # ‚ùå managed_prometheus can cause issues
  monitoring_config {
    managed_prometheus {
      enabled = true
    }
  }
}
```

**‚úÖ CORRECT - Use Secondary Ranges:**
```hcl
resource "google_container_cluster" "primary" {
  # ‚úÖ Use secondary ranges from subnet
  ip_allocation_policy {
    cluster_secondary_range_name  = "pods"
    services_secondary_range_name = "services"
  }
  
  # ‚úÖ Simple monitoring config
  monitoring_config {
    enable_components = ["SYSTEM_COMPONENTS", "WORKLOADS"]
  }
  
  # ‚úÖ No network_policy block needed (Dataplane V2 handles it)
}
```

#### **2. Cloud SQL Private IP Requires VPC Peering**

**‚ùå WRONG - Missing VPC Peering:**
```hcl
# This will fail with "network doesn't have private services connection"
resource "google_sql_database_instance" "postgres" {
  settings {
    ip_configuration {
      ipv4_enabled    = false
      private_network = google_compute_network.vpc.id
    }
  }
}
```

**‚úÖ CORRECT - Add VPC Peering First:**
```hcl
# Step 1: Reserve IP range for Google services
resource "google_compute_global_address" "private_ip_address" {
  name          = "${var.cluster_name}-private-ip"
  purpose       = "VPC_PEERING"
  address_type  = "INTERNAL"
  prefix_length = 16
  network       = google_compute_network.vpc.id
}

# Step 2: Create VPC peering connection
resource "google_service_networking_connection" "private_vpc_connection" {
  network                 = google_compute_network.vpc.id
  service                 = "servicenetworking.googleapis.com"
  reserved_peering_ranges = [google_compute_global_address.private_ip_address.name]
}

# Step 3: Cloud SQL with dependency
resource "google_sql_database_instance" "postgres" {
  depends_on = [google_service_networking_connection.private_vpc_connection]
  
  settings {
    ip_configuration {
      ipv4_enabled    = false
      private_network = google_compute_network.vpc.id
    }
  }
}
```

#### **3. Secret Manager Replication Syntax**

**‚ùå WRONG - Old Syntax:**
```hcl
resource "google_secret_manager_secret" "db_password" {
  replication {
    automatic = true  # ‚ùå Invalid argument error
  }
}
```

**‚úÖ CORRECT - New Syntax:**
```hcl
resource "google_secret_manager_secret" "db_password" {
  replication {
    auto {}  # ‚úÖ Correct syntax
  }
}
```

#### **4. Required GCP APIs Must Be Enabled**

**Before running Terraform, enable these APIs:**
```bash
gcloud services enable \
  container.googleapis.com \
  compute.googleapis.com \
  sqladmin.googleapis.com \
  secretmanager.googleapis.com \
  cloudresourcemanager.googleapis.com \
  servicenetworking.googleapis.com
```

**Why:** Terraform will fail if it tries to create resources using disabled APIs.

#### **5. Subnet Secondary Ranges for GKE**

**‚ùå WRONG - Missing Secondary Ranges:**
```hcl
resource "google_compute_subnetwork" "subnet" {
  ip_cidr_range = "10.0.0.0/24"
  # ‚ùå No secondary ranges - GKE will fail
}
```

**‚úÖ CORRECT - Define Secondary Ranges:**
```hcl
resource "google_compute_subnetwork" "subnet" {
  ip_cidr_range = "10.0.0.0/24"
  
  # ‚úÖ Secondary range for pods
  secondary_ip_range {
    range_name    = "pods"
    ip_cidr_range = "10.1.0.0/16"
  }
  
  # ‚úÖ Secondary range for services
  secondary_ip_range {
    range_name    = "services"
    ip_cidr_range = "10.2.0.0/22"
  }
}
```

### Terraform State Management

**Critical Rules:**
1. **NEVER commit `terraform.tfstate`** to Git (contains sensitive data)
2. **ALWAYS use remote state** (GCS bucket with versioning)
3. **NEVER run `terraform apply` locally in production** - use CI/CD
4. **Lock state during operations** - Terraform does this automatically with GCS backend

**State Backend Configuration:**
```hcl
# terragrunt.hcl
remote_state {
  backend = "gcs"
  config = {
    bucket   = "expense-tracker-terraform-state-${get_env("GCP_PROJECT_ID")}"
    prefix   = "${path_relative_to_include()}/terraform.tfstate"
    project  = get_env("GCP_PROJECT_ID")
  }
}
```

### Terragrunt Best Practices

**‚úÖ DO:**
- Use `terragrunt.hcl` for DRY configuration
- Keep environment-specific values in `environments/{env}/terragrunt.hcl`
- Use `get_env()` for project-specific variables
- Generate backend and provider configs automatically

**‚ùå DON'T:**
- Hardcode project IDs in Terraform files
- Duplicate infrastructure code across environments
- Commit `terraform.tfvars` with sensitive data
- Use `terraform` directly when you have Terragrunt (use `terragrunt` commands)

### IP Address Planning

**Plan your IP ranges to avoid conflicts:**

```
VPC: 10.0.0.0/16 (65,536 IPs total)
‚îú‚îÄ‚îÄ Primary Subnet: 10.0.0.0/24 (256 IPs for nodes)
‚îú‚îÄ‚îÄ Pods Range: 10.1.0.0/16 (65,536 IPs for pods)
‚îú‚îÄ‚îÄ Services Range: 10.2.0.0/22 (1,024 IPs for services)
‚îî‚îÄ‚îÄ Google Services: 10.20.0.0/16 (65,536 IPs for Cloud SQL, etc.)
```

**Rules:**
- Use `/24` for small networks (256 IPs)
- Use `/16` for large networks (65,536 IPs)
- Don't overlap ranges
- Document your IP allocations

### Cost Optimization for Learning

**For learning/demo projects:**
```hcl
# Staging environment
inputs = {
  node_count     = 1              # Single node
  machine_type   = "e2-small"     # Smallest type
  min_node_count = 1
  max_node_count = 2              # Low max
}

# Use preemptible nodes (80% cheaper)
node_config {
  preemptible = var.environment == "staging" ? true : false
}

# Use smallest database tier
settings {
  tier = var.environment == "staging" ? "db-f1-micro" : "db-custom-2-4096"
}
```

**Stop cluster when not using:**
```bash
gcloud container clusters resize CLUSTER_NAME --num-nodes=0
```

### Debugging Terraform Errors

**Common error patterns:**

1. **"Request contains an invalid argument"**
   - Check for deprecated configurations
   - Verify CIDR notation is correct
   - Remove unsupported features

2. **"Network doesn't have private services connection"**
   - Add VPC peering resources
   - Ensure `depends_on` is set correctly

3. **"API not enabled"**
   - Run `gcloud services enable <api>`
   - Wait 1-2 minutes for API to activate

4. **"State lock"**
   - Another operation is running
   - Wait for it to complete
   - Or force unlock: `terragrunt force-unlock LOCK_ID`

### Security Best Practices

1. **Use Workload Identity** for GKE pod authentication (no service account keys)
2. **Private IPs only** for databases (no public exposure)
3. **Secret Manager** for passwords and API keys
4. **IAM least privilege** - grant minimum necessary permissions
5. **Deletion protection** for production resources

### GKE Service Account IAM Roles (CRITICAL)

**ALWAYS include these IAM roles for GKE service accounts:**

```hcl
resource "google_project_iam_member" "gke_sa_roles" {
  for_each = toset([
    "roles/logging.logWriter",              # Required for logging
    "roles/monitoring.metricWriter",        # Required for metrics
    "roles/monitoring.viewer",              # Required for monitoring
    "roles/stackdriver.resourceMetadata.writer",  # Required for Stackdriver
    "roles/artifactregistry.reader",        # ‚ö†Ô∏è REQUIRED to pull images from Artifact Registry
    # "roles/storage.objectViewer",         # Only if using GCR (deprecated)
  ])
  
  project = var.project_id
  role    = each.value
  member  = "serviceAccount:${google_service_account.gke_sa.email}"
}
```

**Why `roles/artifactregistry.reader` is critical:**
- Without this role, GKE nodes **cannot pull Docker images** from Artifact Registry
- Pods will fail with `ImagePullBackOff` and `403 Forbidden` errors
- This is a common mistake when migrating from GCR to Artifact Registry

**For GCR (deprecated):**
- Use `roles/storage.objectViewer` instead
- But prefer Artifact Registry for new projects

### Documentation Requirements

**Every Terraform project should have:**
- ‚úÖ `README.md` - Setup instructions
- ‚úÖ `NETWORKING.md` - IP ranges and architecture
- ‚úÖ `TERRAGRUNT_EXPLAINED.md` - How Terragrunt works
- ‚úÖ `GITHUB_WORKFLOWS_EXPLAINED.md` - CI/CD process
- ‚úÖ `.env.example` or `terraform.tfvars.example` - Required variables
- ‚úÖ Architecture diagrams (ASCII or images)

### Pre-Deployment Checklist

Before running `terragrunt apply`:

- [ ] GCP project created
- [ ] Billing enabled
- [ ] Required APIs enabled
- [ ] GCS bucket for state created
- [ ] `GCP_PROJECT_ID` environment variable set
- [ ] Authenticated with `gcloud auth application-default login`
- [ ] Reviewed `terragrunt plan` output
- [ ] IP ranges don't overlap
- [ ] Cost estimate acceptable

### Post-Deployment Verification

After `terragrunt apply` succeeds:

```bash
# Verify GKE cluster
gcloud container clusters list
kubectl get nodes

# Verify Cloud SQL
gcloud sql instances list

# Verify networking
gcloud compute networks list
gcloud compute networks subnets list

# Check outputs
terragrunt output
```

### 6. Kubernetes (K8s) Configuration

**Objective**: Define application deployment, configuration, and scaling with Kubernetes

**Actions**:
- Create Kubernetes manifests in `k8s/`:

k8s/ 
‚îú‚îÄ‚îÄ base/ # Base configurations 
‚îÇ ‚îú‚îÄ‚îÄ backend-deployment.yaml 
‚îÇ ‚îú‚îÄ‚îÄ backend-service.yaml 
‚îÇ ‚îú‚îÄ‚îÄ frontend-deployment.yaml 
‚îÇ ‚îú‚îÄ‚îÄ frontend-service.yaml 
‚îÇ ‚îú‚îÄ‚îÄ configmap.yaml 
‚îÇ ‚îú‚îÄ‚îÄ secret.yaml # Reference only, actual secrets from Secret Manager 
‚îÇ ‚îú‚îÄ‚îÄ ingress.yaml 
‚îÇ ‚îî‚îÄ‚îÄ kustomization.yaml 
‚îî‚îÄ‚îÄ overlays/ # Environment-specific overrides 
‚îú‚îÄ‚îÄ staging/ 
‚îÇ ‚îú‚îÄ‚îÄ kustomization.yaml 
‚îÇ ‚îú‚îÄ‚îÄ replica-count.yaml 
‚îÇ ‚îî‚îÄ‚îÄ ingress.yaml 
‚îî‚îÄ‚îÄ production/ 
‚îú‚îÄ‚îÄ kustomization.yaml 
‚îú‚îÄ‚îÄ replica-count.yaml 
‚îú‚îÄ‚îÄ hpa.yaml # Horizontal Pod Autoscaler 
‚îî‚îÄ‚îÄ ingress.yaml

- **Use Kustomize** for environment-specific configurations

## Kustomize Best Practices

### Modern Syntax (Use These):
```yaml
# ‚úÖ CORRECT - Modern Kustomize syntax
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:           # ‚Üê Use 'resources' (not 'bases')
  - ../../base

patches:             # ‚Üê Use 'patches' (not 'patchesStrategicMerge')
  - path: replica-patch.yaml
```

### Deprecated Syntax (Don't Use):
```yaml
# ‚ùå DEPRECATED - Old syntax
bases:                      # Deprecated, use 'resources'
  - ../../base

patchesStrategicMerge:      # Deprecated, use 'patches'
  - replica-patch.yaml
```

### Patch Files Must Include Namespace

**‚ùå WRONG - Missing namespace:**
```yaml
# This will fail with "no matches for Id Deployment.v1.apps/backend.[noNs]"
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  # Missing namespace!
spec:
  replicas: 1
```

**‚úÖ CORRECT - Include namespace:**
```yaml
# Patch must match the namespace in base manifests
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: expense-tracker  # ‚Üê Must match base namespace
spec:
  replicas: 1
```

**Why:** Kustomize matches patches to base resources by `apiVersion`, `kind`, `name`, AND `namespace`. If namespace is missing, Kustomize can't find the target resource to patch.

### Kustomize Workflow

1. **Preview changes before applying:**
   ```bash
   kubectl kustomize k8s/overlays/staging/
   ```
   - Shows final YAML that will be applied
   - Verifies patches are working correctly
   - Checks namespace transformations

2. **Apply with Kustomize:**
   ```bash
   kubectl apply -k k8s/overlays/staging/
   ```
   - Applies all resources in one command
   - Handles dependencies automatically

3. **Delete with Kustomize:**
   ```bash
   kubectl delete -k k8s/overlays/staging/
   ```
   - Deletes all resources created by Kustomize

### Common Kustomize Errors

**Error: "no matches for Id Deployment.v1.apps/backend.[noNs]"**
- **Cause:** Patch file missing `namespace` field
- **Fix:** Add `namespace: <base-namespace>` to patch metadata

**Error: "'bases' is deprecated"**
- **Cause:** Using old Kustomize syntax
- **Fix:** Change `bases:` to `resources:`

**Error: "'patchesStrategicMerge' is deprecated"**
- **Cause:** Using old patch syntax
- **Fix:** Change `patchesStrategicMerge:` to `patches:` with `path:` field
- Configure health probes:
  - **Liveness probe**: `/actuator/health/liveness` for backend
  - **Readiness probe**: `/actuator/health/readiness` for backend
  - Frontend: HTTP GET on root path
- Set resource requests and limits:
  - CPU: requests (guaranteed) and limits (max)
  - Memory: requests and limits
- Configure Horizontal Pod Autoscaler (HPA) for production
- Set up Ingress with managed SSL certificates
- Use External Secrets Operator to sync secrets from GCP Secret Manager
- Configure Network Policies for security

**Best Practices**:
- Always set resource limits to prevent resource starvation
- Use health probes to ensure K8s knows when pods are ready
- Use ConfigMaps for non-sensitive config, Secrets/External Secrets for sensitive data
- Label everything consistently for easier management
- Use namespaces to separate environments if sharing clusters

**Deliverables**:
- ‚úÖ Kubernetes manifests in `k8s/base/` and `k8s/overlays/`
- ‚úÖ Kustomize configurations for staging and production
- ‚úÖ Health probes configured on all deployments
- ‚úÖ Resource requests and limits defined
- ‚úÖ HPA configured for production
- ‚úÖ Ingress with SSL termination
- ‚úÖ External Secrets Operator configured

### 7. Continuous Deployment (CD) Pipeline

**Objective**: Automated deployment to staging and production

**Actions**:
- Create `.github/workflows/cd-staging.yml`:
  - Trigger: Push to `develop` branch or manual workflow dispatch
  - Build and tag Docker images with commit SHA
  - Push images to GCR (Google Container Registry)
  - Deploy to staging GKE cluster using `kubectl` and Kustomize
  - Run smoke tests against staging
  - Notify team of deployment status (Slack, email)
- Create `.github/workflows/cd-production.yml`:
  - Trigger: Push to `main` branch, tags, or manual workflow dispatch
  - Require manual approval (GitHub Environments)
  - Build and tag Docker images
  - Push images to GCR
  - Deploy to production GKE cluster
  - Run smoke tests
  - Monitor rollout status
  - Notify team
- Configure GitHub Environments:
  - `staging`: Auto-deploy, no approvals needed
  - `production`: Require manual approval from designated reviewers
- Set up deployment strategies:
  - Rolling updates (default)
  - Consider blue-green or canary for critical services
- Implement rollback procedures:
  - `kubectl rollout undo deployment/<name>`
  - Or redeploy previous version via Git tag

**GitOps Alternative** (Optional):
- Use ArgoCD or Flux for GitOps-based deployments
- K8s cluster pulls changes from Git (vs. CI pushing to cluster)
- Better audit trail and declarative state management

**Deliverables**:
- ‚úÖ `.github/workflows/cd-staging.yml` deploys to staging on `develop` branch
- ‚úÖ `.github/workflows/cd-production.yml` deploys to production on `main` branch
- ‚úÖ GitHub Environments configured with protection rules
- ‚úÖ Automated smoke tests run post-deployment
- ‚úÖ Rollback procedures documented and tested
- ‚úÖ Deployment notifications set up

### 8. Observability and Monitoring

**Objective**: Gain visibility into application performance, errors, and infrastructure health

**Why Critical?** You can't fix what you can't see. Observability is essential for production systems.

**Actions**:
- **Logging**:
  - Structured logging (JSON format) in all services
  - Use appropriate log levels (DEBUG, INFO, WARN, ERROR)
  - Centralize logs: GCP Cloud Logging, ELK Stack, or Grafana Loki
  - Include correlation IDs for tracing requests across services
- **Metrics**:
  - Expose metrics via Spring Boot Actuator (`/actuator/metrics`)
  - Set up Prometheus to scrape metrics from K8s pods
  - Create Grafana dashboards for:
    - Application metrics (request rate, error rate, latency)
    - Infrastructure metrics (CPU, memory, disk, network)
    - Business metrics (user signups, transactions, etc.)
- **Tracing** (Optional, but recommended for microservices):
  - Implement distributed tracing with OpenTelemetry
  - Use Jaeger or Zipkin for trace visualization
  - Trace requests across frontend ‚Üí backend ‚Üí database
- **Alerting**:
  - Set up alerts for critical issues:
    - High error rates
    - Slow response times
    - Resource exhaustion (CPU, memory, disk)
    - Pod crashes or restarts
  - Configure alert channels (Slack, PagerDuty, email)
  - Define on-call rotation and escalation policies
- **Uptime Monitoring**:
  - External monitoring (Pingdom, UptimeRobot, or GCP Monitoring)
  - Monitor critical endpoints and user flows
  - Alert immediately on downtime

**Deliverables**:
- ‚úÖ Structured logging implemented in all services
- ‚úÖ Logs centralized and searchable
- ‚úÖ Prometheus and Grafana deployed
- ‚úÖ Dashboards created for application and infrastructure metrics
- ‚úÖ Alerts configured for critical issues
- ‚úÖ On-call rotation and escalation procedures defined
- ‚úÖ Distributed tracing implemented (if applicable)

## Development Workflow Summary

**Daily Development Flow**:
1. Pull latest code from `develop`
2. Create feature branch: `git checkout -b feature/my-feature`
3. Develop locally using `docker-compose -f docker-compose.dev.yml up`
4. Write tests alongside code (TDD)
5. Run tests locally: `npm test` and `mvn test`
6. Commit and push: CI runs automatically
7. Create Pull Request when feature is complete
8. Address review comments and ensure CI passes
9. Merge to `develop` ‚Üí Auto-deploys to staging
10. Verify on staging, then merge `develop` ‚Üí `main` ‚Üí Deploys to production

**Branch Strategy**:
- `main`: Production-ready code, protected, deploys to production
- `develop`: Integration branch, protected, deploys to staging  
- `feature/*`: Feature branches, created from `develop`, merged back via PR
- `hotfix/*`: Emergency fixes, created from `main`, merged to both `main` and `develop`

**Release Process**:
1. Merge features to `develop` throughout the sprint
2. Test on staging environment
3. Create release PR: `develop` ‚Üí `main`
4. Run final QA and approval process
5. Merge to `main` ‚Üí Triggers production deployment
6. Tag release: `git tag v1.0.0` ‚Üí Push tag
7. Monitor production metrics and logs
8. Celebrate! üéâ

## Key Takeaways

- **CI/CD from Day One**: Don't wait to set up automation
- **Test Early and Often**: Testing is not a separate phase, it's integrated
- **Infrastructure Before Applications**: Provision infrastructure before deploying apps
- **Security Throughout**: Secrets management, scanning, and least privilege from the start
- **Observability is Essential**: You must be able to see what's happening in production
- **Terragrunt for Multi-Environment**: Use Terragrunt to manage Terraform across environments
- **Automate Everything**: Manual processes are error-prone and don't scale

# Preferred Libraries and Packages

## Backend
- **Spring Data JPA**: For simplifying data access layers and database interactions.
- **Lombok**: To reduce boilerplate code (getters, setters, constructors) in Java classes.
- **Testcontainers**: For reliable integration tests with real services like PostgreSQL running in Docker.
- **Spring Boot Actuator**: For production-ready features like health checks and metrics.
- **JaCoCo**: For code coverage reporting and enforcement (configured in `pom.xml`)

## Frontend
- **Shadcn**: For building accessible and composable UI components. Built on top of Radix UI and Tailwind CSS.
- **Axios**: For making HTTP requests from the browser to the backend API.
- **Tailwind CSS**: For utility-first CSS styling.
- **React Testing Library**: For testing React components in a user-centric way.

## Shadcn UI Component Library

- **ALWAYS use `shadcn` (modern version)** - NOT the deprecated `shadcn-ui` package
- **Correct package**: `shadcn` is installed via `npx shadcn@latest init` and components are added with `npx shadcn@latest add <component>`
- **Import path**: Components should be imported from `@/components/ui/*` (e.g., `import { Button } from "@/components/ui/button"`)
- **NEVER install or reference `shadcn-ui`** - This is the old, deprecated package name
- Shadcn components are copied into your project (not installed as npm dependencies) and can be customized
- Built on Radix UI primitives with Tailwind CSS styling

# Spring Boot Best Practices

- **Dependency Management**: Always use the `spring-boot-starter-parent` POM to ensure dependency compatibility.
- **Configuration**: Use `application.properties` or `application.yml` for configuration. Use profiles (`application-dev.yml`, `application-prod.yml`) for environment-specific settings.
- **Testing**: 
  - Use `@SpringBootTest` for full integration tests.
  - Use `@DataJpaTest` for testing the persistence layer with an in-memory database like H2.
  - Use Testcontainers for integration tests that require external services like PostgreSQL to ensure production parity.
  - Use `@WebMvcTest` for controller tests with mocked services.

# Maven Best Practices

## Maven Lifecycle Commands

**ALWAYS use Maven lifecycle phases, NOT direct plugin goals:**

### ‚úÖ Correct Commands (Use These):
```bash
mvn clean test          # Run tests only
mvn clean verify        # Run tests + coverage checks (RECOMMENDED for CI)
mvn clean package       # Run verify + create JAR
mvn clean install       # Run package + install to local Maven repo
```

### ‚ùå Wrong Commands (Don't Use):
```bash
mvn clean test jacoco:report jacoco:check    # Direct goals ignore pom.xml config
mvn jacoco:check                              # Missing rules configuration
```

## Why Lifecycle Phases Are Better

- **Lifecycle phases** (test, verify, package) automatically trigger configured `<execution>` blocks in `pom.xml`
- **Direct plugin goals** (jacoco:check) run standalone and ignore `<execution>` configuration
- This means direct goals won't use your exclusions, rules, or thresholds

## Maven Phase Order
```
clean ‚Üí compile ‚Üí test ‚Üí package ‚Üí verify ‚Üí install ‚Üí deploy
```

## Code Coverage with JaCoCo

### JaCoCo Configuration Pattern:
- **Exclude** data containers from coverage: `**/dto/**`, `**/model/**`, `**/config/**`, Main Application class
- **Set thresholds** in `<execution id="check">`: LINE 70%, BRANCH 50%
- **Runs automatically** during `mvn verify` via `<executions>` blocks in `pom.xml`
- **Focus coverage on** business logic: Services, Controllers, Repositories
- See `backend/pom.xml` for full JaCoCo plugin configuration

## CI/CD Maven Commands

### For GitHub Actions / CI Pipelines:
```yaml
# ‚úÖ CORRECT - Runs tests + coverage check
- name: Run backend tests with coverage
  run: mvn clean verify

# ‚ùå WRONG - Doesn't use pom.xml configuration
- name: Run backend tests with coverage
  run: mvn clean test jacoco:report jacoco:check
```

### Local Development:
```bash
# Run tests with coverage
mvn clean verify

# View coverage report
open target/site/jacoco/index.html

# Skip tests (for quick builds)
mvn clean package -DskipTests
```

# Security Best Practices

## Environment Variables and Secrets
- **NEVER hardcode sensitive data** (passwords, API keys, tokens, secrets) in source code
- **ALWAYS use environment variables** for sensitive configuration
- **Backend (Spring Boot)**: Use `${VAR_NAME:default}` syntax in `application.properties`
  - Example: `spring.datasource.password=${DB_PASSWORD:postgres}`
- **Frontend (Next.js)**: Use `process.env.NEXT_PUBLIC_*` for client-side, `process.env.*` for server-side
- **Docker Compose**: Reference environment variables with `${VAR_NAME}` syntax
- **NEVER commit `.env` files** - only commit `.env.example` with placeholder values
- **Kubernetes**: Use Secrets or External Secrets Operator, never hardcode in YAML
- **Always validate** that `.env` is in `.gitignore`
- For local development, use `.env` files; for production, use secret management services (GCP Secret Manager, AWS Secrets Manager, etc.)

# React and Next.js Best Practices

## React Imports
- **NEVER use `import * as React from "react"`** - This causes build failures in Next.js Docker environments
- **NEVER use `React.` namespace references** (e.g., `React.FormEvent`, `React.ReactNode`) without importing React
- **ALWAYS use named imports**: `import { useState, useEffect, forwardRef } from "react"`
- **ALWAYS use type imports for React types**: `import type { ReactNode, FormEvent, HTMLAttributes } from "react"`
- Common type imports needed:
  - `FormEvent` for form submit handlers (instead of `React.FormEvent`)
  - `ReactNode` for children props (instead of `React.ReactNode`)
  - `ChangeEvent` for input change handlers (instead of `React.ChangeEvent`)
  - `MouseEvent` for click handlers (instead of `React.MouseEvent`)

## Client Components
- All components using React hooks or browser APIs must have `"use client"` directive at the top
- UI components that use `forwardRef`, `useState`, `useEffect`, etc. need `"use client"`

## React Hooks Dependencies
- Always include all dependencies in `useEffect`, `useCallback`, and `useMemo` dependency arrays
- Wrap functions used in `useEffect` with `useCallback` to prevent infinite loops
- Example:
  ```typescript
  const loadData = useCallback(async () => {
    // fetch logic
  }, [toast]); // Include all external dependencies
  
  useEffect(() => {
    loadData();
  }, [loadData]); // Include the callback function
  ```

# Frontend Testing Best Practices (Jest + React Testing Library)

## Jest Configuration

### Required Files:
1. **`jest.config.js`** - Main Jest configuration with Next.js integration
2. **`jest.setup.js`** - Test environment setup (imports `@testing-library/jest-dom`)
3. **`__mocks__/axios.js`** - Manual mock for axios (see Axios Mocking section)

### jest.config.js Pattern:
```javascript
const nextJest = require('next/jest')

const createJestConfig = nextJest({
  dir: './',
})

const customJestConfig = {
  setupFilesAfterEnv: ['<rootDir>/jest.setup.js'],
  testEnvironment: 'jest-environment-jsdom',
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/src/$1',
  },
  collectCoverageFrom: [
    'src/**/*.{js,jsx,ts,tsx}',
    '!src/**/*.d.ts',
    '!src/app/**',           // Exclude Next.js app directory
    '!src/components/**',    // Exclude components until tested
  ],
  coverageThreshold: {
    global: {
      lines: 60,
      branches: 40,
      functions: 55,
      statements: 60,
    },
  },
}

module.exports = createJestConfig(customJestConfig)
```

## Axios Mocking (CRITICAL - Common Bug)

### The Problem:
Axios instances created with `axios.create()` at module load time are difficult to mock because they're initialized before tests run.

### The Solution - Factory Function Pattern:
```typescript
// __mocks__/axios.js
export default {
  create: jest.fn(() => ({
    get: jest.fn(),
    post: jest.fn(),
    put: jest.fn(),
    delete: jest.fn(),
    patch: jest.fn(),
  })),
  get: jest.fn(),
  post: jest.fn(),
  put: jest.fn(),
  delete: jest.fn(),
  patch: jest.fn()
}
```

```typescript
// api.test.ts
import axios from 'axios'
import { RecurrenceFrequency } from './api'

// Mock axios with factory function
jest.mock('axios', () => {
  const mockInstance = {
    get: jest.fn(),
    post: jest.fn(),
    put: jest.fn(),
    delete: jest.fn(),
    patch: jest.fn()
  }
  
  return {
    __esModule: true,
    default: {
      create: jest.fn(() => mockInstance)
    }
  }
})

// Import API after mocking
const { expenseApi } = require('./api')

// Get reference to the mock instance
const mockAxiosInstance = (axios.create as jest.Mock).mock.results[0].value

describe('API Tests', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })

  it('should call API', async () => {
    mockAxiosInstance.get.mockResolvedValueOnce({ data: mockData })
    const result = await expenseApi.getAllExpenses()
    expect(mockAxiosInstance.get).toHaveBeenCalledWith('/expenses')
  })
})
```

### Key Points:
- ‚úÖ **Create mock instance inside factory function** - Avoids "Cannot access before initialization" error
- ‚úÖ **Use `require()` to import API** - Ensures import happens after mock setup
- ‚úÖ **Get mock instance from `mock.results[0].value`** - Access the created instance
- ‚ùå **Don't define mock instance outside factory** - Causes circular reference
- ‚ùå **Don't use `import` for API** - Import hoisting breaks mocking

## Test File Organization

### Utility Functions (`utils.test.ts`):
```typescript
import { formatCurrency, formatDate, cn } from './utils'

describe('Utils', () => {
  describe('formatCurrency', () => {
    it('should format positive numbers', () => {
      expect(formatCurrency(1234.56)).toBe('$1,234.56')
    })
  })
})
```

### API Layer (`api.test.ts`):
- Test all CRUD operations
- Test both success and error paths
- Mock axios responses
- Verify correct endpoints called

### Components (React Testing Library):
```typescript
import { render, screen, fireEvent } from '@testing-library/react'
import { Button } from './button'

describe('Button', () => {
  it('should render and handle click', () => {
    const handleClick = jest.fn()
    render(<Button onClick={handleClick}>Click me</Button>)
    
    fireEvent.click(screen.getByText('Click me'))
    expect(handleClick).toHaveBeenCalledTimes(1)
  })
})
```

## Coverage Configuration

### Exclude from Coverage:
- **Components** - Until component tests are written
- **App directory** - Next.js routing (framework code)
- **UI library components** - External libraries (shadcn)
- **Type definitions** - `.d.ts` files

### Coverage Thresholds:
- Start conservative: 60% lines, 40% branches, 55% functions, 60% statements
- Gradually increase as tests are added
- Define in `jest.config.js`, not in CI command line
- CI runs: `npm test -- --coverage` (thresholds from config)

## CI/CD Integration

```yaml
- name: Run frontend tests with coverage
  working-directory: ./frontend
  run: npm test -- --coverage
  # Coverage thresholds defined in jest.config.js
```

**Don't** use inline `--coverageThreshold` in CI - define in `jest.config.js` instead.

## State Management and Optimistic Updates

### Avoid Race Conditions with Async State Updates
- **NEVER call `await loadData()` immediately after a mutation** - This creates race conditions where state can be undefined during re-renders
- **ALWAYS use optimistic updates** for mutations (create, update, delete operations)
- **Use functional state updates** `setState((current) => ...)` to ensure you're working with the latest state
- **Pattern to avoid**:
  ```typescript
  // ‚ùå BAD: Race condition - state can be inconsistent during reload
  await api.updateItem(id, data);
  await loadItems(); // Causes re-render with potentially undefined state
  ```

- **Pattern to use**:
  ```typescript
  // ‚úÖ GOOD: Optimistic update with functional state
  const handleUpdate = async (id: string, data: UpdateData) => {
    // Optimistically update UI
    setItems((current) => 
      current.map((item) => item.id === id ? { ...item, ...data } : item)
    );
    
    try {
      await api.updateItem(id, data);
      toast.success('Updated successfully');
    } catch (error) {
      // Revert on error
      await loadItems();
      toast.error('Update failed');
    }
  };
  ```

## TypeScript with React
- Use type imports for React types: `type ReactNode`, `type HTMLAttributes`
- Prefer `forwardRef<HTMLElement, Props>` over `React.forwardRef`
- Use `ElementRef<typeof Component>` and `ComponentPropsWithoutRef<typeof Component>` for Radix UI components

## Next.js Specific
- Ensure `output: 'standalone'` is set in `next.config.js` for Docker deployments
- All client-side interactive components must be marked with `"use client"`
- Server components should NOT have `"use client"` directive

# Docker Best Practices Summary

## Production Builds
- **Use multi-stage builds** for compiled applications (Java/Spring Boot, Node.js)
- **Standardize on minimal runtime images**: `eclipse-temurin:17-jre` for Java, `node:20-alpine` for Node
- **DO NOT use Maven Wrapper** in Docker builds - use official `maven:3.9-eclipse-temurin-17` image
- Copy dependency files first for better layer caching (e.g., `pom.xml`, `package.json`, `package-lock.json`)

## Frontend Specific (Next.js)
- **ALWAYS copy `package-lock.json` explicitly** in Dockerfile: `COPY package-lock.json ./`
- **Use `npm ci --legacy-peer-deps`** instead of `npm install` for reproducible builds
- Never use conditional logic for package-lock.json - always ensure it exists
- Ensure `.dockerignore` does NOT exclude `package-lock.json`
- **CRITICAL: Ensure `public` directory exists before build** - Add `RUN mkdir -p ./public` in builder stage before `npm run build`
  - Next.js standalone builds fail if public directory doesn't exist when copying in production stage
  - This prevents "not found" errors during Docker COPY operations

## Development Workflow
- **Maintain separate configs** for dev and prod: `Dockerfile` vs `Dockerfile.dev`, `docker-compose.yml` vs `docker-compose.dev.yml`
- Use volume mounts in dev for hot-reload: `./src:/app/src`
- Enable file watching in Docker: `WATCHPACK_POLLING=true` for frontend, Spring DevTools for backend
- Use named volumes for dependencies to persist across restarts: `node_modules`, `.m2`

## Common Issues
- **Missing package-lock.json**: Always commit to version control
- **Peer dependency conflicts**: Use `--legacy-peer-deps` flag with npm ci
- **Cache invalidation**: Copy package files before source files to leverage Docker layer caching

# Common Deployment Issues and Solutions

## Issue 1: Image Pull Failures (403 Forbidden)

**Symptom:**
```
Failed to pull image: 403 Forbidden
Error: ErrImagePull / ImagePullBackOff
```

**Root Cause:** GKE service account lacks permission to pull from Artifact Registry.

**Solution:**
Always grant `roles/artifactregistry.reader` to the GKE service account:

```hcl
# In terraform/main.tf
resource "google_project_iam_member" "gke_sa_roles" {
  for_each = toset([
    "roles/logging.logWriter",
    "roles/monitoring.metricWriter",
    "roles/monitoring.viewer",
    "roles/stackdriver.resourceMetadata.writer",
    "roles/artifactregistry.reader"  # REQUIRED for image pulls
  ])
  project = var.project_id
  role    = each.key
  member  = "serviceAccount:${google_service_account.gke_sa.email}"
}
```

---

## Issue 2: Stale Images (ImagePullPolicy)

**Symptom:** New images pushed but pods still use old images.

**Root Cause:** Default `imagePullPolicy: IfNotPresent` caches images by tag.

**Solution:**
Set `imagePullPolicy: Always` in deployments when reusing tags:

```yaml
# In k8s/base/*-deployment.yaml
containers:
- name: backend
  image: gcr.io/PROJECT_ID/backend:v1.0.0
  imagePullPolicy: Always  # Force pull on every pod creation
```

**Best Practice:**
- **Development:** Use `imagePullPolicy: Always` with tags like `v1.0.0`
- **Production:** Use immutable tags (e.g., git SHA) with `IfNotPresent`

---

## Issue 3: Database Schema Validation Failures

**Symptom:**
```
Schema-validation: missing table [table_name]
```

**Root Cause:** Hibernate set to `validate` mode but database is empty.

**Solution:**
Use appropriate `ddl-auto` setting for each environment:

```yaml
# application-dev.yml (local development)
spring:
  jpa:
    hibernate:
      ddl-auto: create-drop  # Recreate schema on startup

# application-prod.yml (first deployment)
spring:
  jpa:
    hibernate:
      ddl-auto: update  # Auto-create/update tables

# application-prod.yml (after initial deployment)
spring:
  jpa:
    hibernate:
      ddl-auto: validate  # Only validate, don't modify
```

**Best Practice:**
1. First deployment: Use `update` to create schema
2. After stable: Switch to `validate` for safety
3. Use database migration tools (Flyway/Liquibase) for production

---

## Issue 4: Ingress Static IP Mismatch

**Symptom:**
```
Error: the given static IP name 'X' doesn't translate to an existing static IP
```

**Root Cause:** Kubernetes Ingress annotation references wrong static IP name.

**Solution:**
Use Kustomize patches to override annotations per environment:

```yaml
# k8s/overlays/staging/kustomization.yaml
patches:
  - target:
      kind: Ingress
      name: expense-tracker-ingress
    patch: |-
      - op: replace
        path: /metadata/annotations/kubernetes.io~1ingress.global-static-ip-name
        value: expense-tracker-staging-ingress-ip  # Match Terraform output
```

**Best Practice:**
- Ensure Terraform static IP names match Kubernetes ingress annotations
- Use environment-specific patches in Kustomize overlays
- Verify with: `gcloud compute addresses list --global`

---

## Issue 5: Terragrunt Environment Variables

**Symptom:**
```
Error: bucket doesn't exist
Error: get_env("GCP_PROJECT_ID") returns empty string
```

**Root Cause:** Environment variables not persisting between commands.

**Solution:**
Hardcode environment-specific values in `terragrunt.hcl`:

```hcl
# ‚ùå BAD - Requires manual env var
remote_state {
  config = {
    bucket = "terraform-state-${get_env("GCP_PROJECT_ID")}"
    project = get_env("GCP_PROJECT_ID")
  }
}

# ‚úÖ GOOD - Hardcoded per environment
# terraform/environments/staging/terragrunt.hcl
inputs = {
  project_id = "sol-sand"
  environment = "staging"
}

# terraform/terragrunt.hcl (root)
remote_state {
  config = {
    bucket = "expense-tracker-terraform-state-sol-sand"
    project = "sol-sand"
  }
}
```

**Best Practice:**
- Use `inputs` in environment-specific `terragrunt.hcl` files
- Avoid `get_env()` for critical configuration
- Document required variables in README

---

# Best Practices Checklist

When creating a new project or service:
- [ ] Kubernetes manifests created in `k8s/` directory
- [ ] GitHub Actions workflows for CI in `.github/workflows/`
- [ ] Terraform configurations for infrastructure in `terraform/`
- [ ] **GKE service account has `roles/artifactregistry.reader`**
- [ ] **Deployments use `imagePullPolicy: Always` for development**
- [ ] **Hibernate `ddl-auto` set appropriately per environment**
- [ ] **Ingress static IP names match Terraform outputs**
- [ ] **Terragrunt uses hardcoded values, not environment variables**
- [ ] External Secrets configured to pull from GCP Secret Manager
- [ ] Health probes and resource limits defined in K8s deployments
- [ ] Multi-stage Dockerfiles for optimized image builds
- [ ] Horizontal Pod Autoscalers configured for scalability
- [ ] Ingress with managed SSL certificates configured
- [ ] `.env.example` file with all required environment variables
- [ ] Documentation for deployment process in `DEPLOYMENT.md`